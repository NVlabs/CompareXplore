# SPDX-FileCopyrightText: Copyright (c) 2024 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
# SPDX-License-Identifier: Apache-2.0
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from saver import saver
from config import FLAGS
from utils import OurTimer, format_seconds, get_log_path, load_pickle, print_stats, load

from glob import glob
from os.path import join, dirname


class OpenAIIOCache:
    def __init__(self, cache_type):
        self.cache_type = cache_type
        assert self.cache_type in ['embedding', 'comparison']
        self.hit = 0
        self.miss = 0
        self.content = {}
        log_dir = get_log_path()
        files = glob(
            join(log_dir, '**', 'openai_io_collector.pickle'), recursive=True
        )  # TODO: handle different GPT versions
        saver.log_info(
            f'Searching openai_io_collector.pickle in the log folder: Found {len(files)} files'
        )
        # exit()
        for file in files:
            FLAGS_pn = join(dirname(dirname(file)), 'FLAGS.klepto')
            loaded_flags = load(FLAGS_pn)['FLAGS']
            # Critical! Otherwise, may load WRONG io-pairs generated by another GPT model.
            if self._right_file_to_load(loaded_flags):
                loaded = load_pickle(file)['openai_io_collector']
                saver.log_info(f'\tLoaded {len(loaded)} io-pairs from {file}')
                for d in loaded:
                    self._load_into_cache(d)

    def _right_file_to_load(self, loaded_flags):
        if self.cache_type == 'embedding':
            return (
                loaded_flags.model == 'our'
                and loaded_flags.OpenAI_embedding_model == FLAGS.OpenAI_embedding_model
            )
        elif self.cache_type == 'comparison':
            return (
                loaded_flags.model == 'OpenAI'
                and loaded_flags.OpenAI_model == FLAGS.OpenAI_model
            )
        else:
            assert False

    def _load_into_cache(self, d):
        if self.cache_type == 'embedding':
            self.content[d['txt']] = d['embedding']
        elif self.cache_type == 'comparison':
            immutable_messages = self._turn_into_immutable_messages(d['messages'])
            # saver.log_info_at_most(
            #     f'immutable_messages={immutable_messages}',
            #     f'immutable_messages_example',
            #     1,
            # )
            self.content[immutable_messages] = d['response']
        else:
            assert False

    def get_from_cache(self, key):
        rtn = self.content.get(self._get_key(key))
        if rtn is None:
            self.miss += 1
        else:
            self.hit += 1
        return rtn
    
    def insert_into_cache(self, key, value):
        self.content[self._get_key(key)] = value

    def _get_key(self, key):
        if self.cache_type == 'embedding':
            txt = key
            return txt
        elif self.cache_type == 'comparison':
            messages = key
            return self._turn_into_immutable_messages(messages)
        else:
            assert False

    def report(self):
        return (
            f'#hit={self.hit}; #miss={self.miss}; len(self.content)={len(self.content)}'
        )

    def _turn_into_immutable_messages(self, messages):
        # Convert the inner dictionaries to immutable
        immutable_messages = tuple(tuple(d.items()) for d in messages)
        return immutable_messages
